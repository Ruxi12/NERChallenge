{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 8210068,
     "sourceType": "datasetVersion",
     "datasetId": 4865313
    },
    {
     "sourceId": 8256457,
     "sourceType": "datasetVersion",
     "datasetId": 4899873
    }
   ],
   "dockerImageVersionId": 30698,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-04-28T20:21:48.737433Z",
     "iopub.execute_input": "2024-04-28T20:21:48.737905Z",
     "iopub.status.idle": "2024-04-28T20:21:48.747040Z",
     "shell.execute_reply.started": "2024-04-28T20:21:48.737866Z",
     "shell.execute_reply": "2024-04-28T20:21:48.746046Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/input/entities/annotated_entities2.json\n/kaggle/input/entttt/annotated_entities.json\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "# Setting environment variable to avoid tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T20:21:48.748868Z",
     "iopub.execute_input": "2024-04-28T20:21:48.749831Z",
     "iopub.status.idle": "2024-04-28T20:21:48.758240Z",
     "shell.execute_reply.started": "2024-04-28T20:21:48.749630Z",
     "shell.execute_reply": "2024-04-28T20:21:48.757518Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    texts = [item['Content'] for item in data]\n",
    "    annotations = [[(ent['start'], ent['end'], ent['label']) for ent in item['entities']] for item in data]\n",
    "    return texts, annotations\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T20:21:48.759921Z",
     "iopub.execute_input": "2024-04-28T20:21:48.760655Z",
     "iopub.status.idle": "2024-04-28T20:21:48.768033Z",
     "shell.execute_reply.started": "2024-04-28T20:21:48.760620Z",
     "shell.execute_reply": "2024-04-28T20:21:48.767350Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def encode_tags(texts, annotations, label_to_id, max_length):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    encoded_inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    labels = []\n",
    "\n",
    "    for i, (text, annotation) in enumerate(zip(texts, annotations)):\n",
    "        # Create a list of -100 to ignore loss calculation for other tokens\n",
    "        doc_labels = [-100] * len(encoded_inputs['input_ids'][i])\n",
    "        \n",
    "        for start, end, label in annotation:\n",
    "            # Convert character positions to token positions\n",
    "            start_token = encoded_inputs.char_to_token(i, start)\n",
    "            end_token = encoded_inputs.char_to_token(i, end - 1)\n",
    "            if start_token is not None and end_token is not None:\n",
    "                # Set labels only for the first token of each word\n",
    "                doc_labels[start_token] = label_to_id[label]\n",
    "                # Ensure subsequent tokens in the same word are set to -100\n",
    "                doc_labels[start_token + 1:end_token + 1] = [-100] * (end_token - start_token)\n",
    "\n",
    "        labels.append(doc_labels)\n",
    "    \n",
    "    # Ensure that all label sequences are padded to the maximum length of the sequences\n",
    "    labels_padded = [label + [-100] * (max_length - len(label)) for label in labels]\n",
    "    encoded_inputs['labels'] = torch.tensor(labels_padded)\n",
    "    return encoded_inputs\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T20:21:48.770709Z",
     "iopub.execute_input": "2024-04-28T20:21:48.771618Z",
     "iopub.status.idle": "2024-04-28T20:21:48.780939Z",
     "shell.execute_reply.started": "2024-04-28T20:21:48.771430Z",
     "shell.execute_reply": "2024-04-28T20:21:48.779981Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    file_path = '/kaggle/input/entities/annotated_entities2.json'\n",
    "    texts, annotations = load_data(file_path)\n",
    "    print(\"Data is loaded\")\n",
    "    label_to_id = {'O': 0, 'PRODUCT': 1, 'NON_PRODUCT': 2}  # Adăugăm NON_PRODUCT în dictionar  # Mapping labels to IDs\n",
    "\n",
    "    # Encoding data\n",
    "    encoded_data = encode_tags(texts, annotations, label_to_id, max_length=128)\n",
    "    dataset = Dataset.from_dict(encoded_data)\n",
    "    print(\"Data is encoded\")\n",
    "    # Specify device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_to_id))\n",
    "    model.to(device)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model\n",
    "    model.save_pretrained('./trained_ner_model')\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T20:21:48.783738Z",
     "iopub.execute_input": "2024-04-28T20:21:48.784590Z",
     "iopub.status.idle": "2024-04-28T20:21:48.796731Z",
     "shell.execute_reply.started": "2024-04-28T20:21:48.784550Z",
     "shell.execute_reply": "2024-04-28T20:21:48.795883Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(text):\n",
    "    model = BertForTokenClassification.from_pretrained('./trained_ner_model')\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist())\n",
    "\n",
    "    return [(token, prediction) for token, prediction in zip(tokens, predictions) if token not in ['[CLS]', '[SEP]'] and prediction != 0]\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T20:21:48.798207Z",
     "iopub.execute_input": "2024-04-28T20:21:48.798903Z",
     "iopub.status.idle": "2024-04-28T20:21:48.809016Z",
     "shell.execute_reply.started": "2024-04-28T20:21:48.798868Z",
     "shell.execute_reply": "2024-04-28T20:21:48.807928Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # Example inference\n",
    "    example_text = \"Check out these amazing new tables and chairs available in our store.\"\n",
    "    product_names = predict(example_text)\n",
    "    print(\"----------------\")\n",
    "    print(product_names)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T20:21:48.810433Z",
     "iopub.execute_input": "2024-04-28T20:21:48.810716Z",
     "iopub.status.idle": "2024-04-28T20:39:42.943739Z",
     "shell.execute_reply.started": "2024-04-28T20:21:48.810693Z",
     "shell.execute_reply": "2024-04-28T20:39:42.942526Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from datasets import load_metric\n",
    "\n",
    "# metric = load_metric(\"seqeval\")\n",
    "# predictions, labels = predict(validation_texts), actual_labels(validation_texts)\n",
    "# metric.compute(predictions=predictions, references=labels)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T20:40:46.398054Z",
     "iopub.execute_input": "2024-04-28T20:40:46.398468Z",
     "iopub.status.idle": "2024-04-28T20:40:46.936847Z",
     "shell.execute_reply.started": "2024-04-28T20:40:46.398434Z",
     "shell.execute_reply": "2024-04-28T20:40:46.935499Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_metric\n\u001B[1;32m      3\u001B[0m metric \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseqeval\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m predictions, labels \u001B[38;5;241m=\u001B[39m predict(\u001B[43mvalidation_texts\u001B[49m), actual_labels(validation_texts)\n\u001B[1;32m      5\u001B[0m metric\u001B[38;5;241m.\u001B[39mcompute(predictions\u001B[38;5;241m=\u001B[39mpredictions, references\u001B[38;5;241m=\u001B[39mlabels)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'validation_texts' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'validation_texts' is not defined",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install seqeval"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T20:40:27.549688Z",
     "iopub.execute_input": "2024-04-28T20:40:27.550678Z",
     "iopub.status.idle": "2024-04-28T20:40:43.540364Z",
     "shell.execute_reply.started": "2024-04-28T20:40:27.550646Z",
     "shell.execute_reply": "2024-04-28T20:40:43.538978Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.6/43.6 kB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=1ae9f85b52ba97f79a35454981299014fae43710c9abd1d3e5c5a45205f73935\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}